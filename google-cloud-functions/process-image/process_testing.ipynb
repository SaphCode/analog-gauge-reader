{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e45b6f64-ce17-4e77-9bd1-07d80dfcb5c7",
   "metadata": {},
   "source": [
    "## Testing image upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa11ba55-6814-47b9-8636-f25f7f8dd85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "image_path = os.path.join(\"../\", \"../\", \"tests\",\"test_image_2.jpg\")\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9735456-93ae-4b31-a6cd-553b2218f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\"detections\":{\"confidence\":{\"center\":0.893,\"gauge\":0.988,\"max\":0.861,\"min\":0.654,\"tip\":0.812},\"found\":[\"gauge\",\"center\",\"max\",\"tip\",\"min\"]},\"device_id\":\"jupyter_test\",\"firestore_id\":\"RsGjORXEda5c5DEKUYr4\",\"measurement\":91.22,\"status\":\"success\",\"storage\":{\"bucket\":\"analog-gauge-images\",\"full_url\":\"gs://analog-gauge-images/jupyter_test/20251111/20251111_132651_016266.jpg.jpg\",\"image_path\":\"jupyter_test/20251111/20251111_132651_016266.jpg.jpg\"},\"timestamp\":\"2025-11-11 13:26:51.016266\",\"unit\":\"PSI\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import io\n",
    "\n",
    "url = \"https://europe-west3-pressure-watcher-ea41a.cloudfunctions.net/process-image\"\n",
    "\n",
    "# Convert image to bytes\n",
    "img_byte_arr = io.BytesIO()\n",
    "image.save(img_byte_arr, format='JPEG')\n",
    "img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "timestamp = datetime.now()\n",
    "filename = f\"{timestamp.strftime(\"%Y%m%d_%H%M%S_%f\")}.jpg\" # Example output: 20240101_123456.png\n",
    "\n",
    "files = {'image': (filename, img_byte_arr, 'image/jpeg')}\n",
    "data = {\n",
    "    'image': filename,\n",
    "    'timestamp': str(timestamp),\n",
    "    'device_id': \"jupyter_test\",\n",
    "    'min_value': 0,\n",
    "    'max_value': 160,\n",
    "    'unit': \"PSI\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, files=files, data=data)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba72ed8-24d5-45b9-a3d5-d925028596c1",
   "metadata": {},
   "source": [
    "# Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43c1afc3-6718-41fc-9910-f769ed03db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "\n",
    "for i in range(30):\n",
    "    for f in os.listdir(\"mock_data\"):\n",
    "        random_choice = np.random.choice([1,2,3])\n",
    "        if random_choice != 3:\n",
    "            if 'ipynb' in f: continue\n",
    "            image_path = os.path.join(\"mock_data\", f)\n",
    "            image = Image.open(image_path)\n",
    "        \n",
    "            # Convert image to bytes\n",
    "            img_byte_arr = io.BytesIO()\n",
    "            image.save(img_byte_arr, format='JPEG')\n",
    "            img_byte_arr = img_byte_arr.getvalue()\n",
    "            \n",
    "            timestamp = datetime.now()\n",
    "            filename = f\"{timestamp.strftime(\"%Y%m%d_%H%M%S_%f\")}.jpg\" # Example output: 20240101_123456.png\n",
    "            \n",
    "            files = {'image': (filename, img_byte_arr, 'image/jpeg')}\n",
    "            data = {\n",
    "                'image': filename,\n",
    "                'timestamp': str(timestamp),\n",
    "                'device_id': \"demo\",\n",
    "                'min_value': -1,\n",
    "                'max_value': 3,\n",
    "                'unit': \"bar\"\n",
    "            }\n",
    "        \n",
    "            response = requests.post(url, files=files, data=data)\n",
    "            \n",
    "            sleep(10)\n",
    "        else:\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09aa68e5-9fa0-46f8-a739-c24e386a632e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOLO Model Inference Module\n",
    "\n",
    "This module provides functionality for loading a YOLO model and detecting\n",
    "objects in images, specifically designed for analog gauge reading applications.\n",
    "\n",
    "Classes detected:\n",
    "    - center: Center point of the gauge\n",
    "    - gauge: The entire gauge body\n",
    "    - max: Maximum value marker on the gauge\n",
    "    - min: Minimum value marker on the gauge\n",
    "    - tip: Tip of the gauge needle\n",
    "\n",
    "Usage:\n",
    "    from yolo import GaugeDetector\n",
    "    \n",
    "    detector = GaugeDetector('path/to/yolo_best.pt')\n",
    "    detections = detector.predict('path/to/image.jpg')\n",
    "    \n",
    "    # Access detections by class name\n",
    "    center_box = detections['center']\n",
    "    tip_box = detections['tip']\n",
    "\"\"\"\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from typing import Dict, Tuple\n",
    "import os\n",
    "from PIL.Image import Image as PILImage\n",
    "\n",
    "\n",
    "class Detection:\n",
    "    \"\"\"\n",
    "    Represents a single object detection with bounding box and metadata.\n",
    "    \n",
    "    Attributes:\n",
    "        class_name (str): Name of the detected class\n",
    "        confidence (float): Detection confidence score (0-1)\n",
    "        bbox (Tuple[float, float, float, float]): Bounding box coordinates (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, class_name: str, confidence: float, bbox: Tuple[float, float, float, float]):\n",
    "        \"\"\"\n",
    "        Initialize a Detection object.\n",
    "        \n",
    "        Args:\n",
    "            class_name: Name of the detected class\n",
    "            confidence: Confidence score between 0 and 1\n",
    "            bbox: Bounding box as (x1, y1, x2, y2) coordinates\n",
    "        \"\"\"\n",
    "        self.class_name = class_name\n",
    "        self.confidence = confidence\n",
    "        self.bbox = bbox\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return (f\"Detection(class='{self.class_name}', \"\n",
    "                f\"confidence={self.confidence:.2%}, \"\n",
    "                f\"bbox={[f'{x:.0f}' for x in self.bbox]})\")\n",
    "\n",
    "\n",
    "class GaugeDetector:\n",
    "    \"\"\"\n",
    "    YOLO-based detector for analog gauge components.\n",
    "    \n",
    "    This class handles loading a YOLO model and running inference on images\n",
    "    to detect gauge components (center, tip, min, max markers).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the GaugeDetector with a trained YOLO model.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the YOLO model weights file (.pt)\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the model file doesn't exist\n",
    "            ValueError: If the model cannot be loaded\n",
    "        \"\"\"\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "        \n",
    "        try:\n",
    "            self.model = YOLO(model_path)\n",
    "            self.class_names = self.model.names\n",
    "            print(\"=\" * 60)\n",
    "            print(\"MODEL LOADED\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Classes: {self.class_names}\")\n",
    "            print(f\"Number of classes: {len(self.class_names)}\")\n",
    "            print(\"=\" * 60)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load model: {e}\")\n",
    "    \n",
    "    def predict(self, image: PILImage, verbose: bool = True) -> Dict[str, Detection]:\n",
    "        \"\"\"\n",
    "        Run inference on an image and return the highest confidence detection for each class.\n",
    "        \n",
    "        This method detects all objects in the image and returns only the detection\n",
    "        with the highest confidence score for each class. This is important for\n",
    "        gauge reading where multiple false positives might occur.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the input image\n",
    "            verbose: If True, print detection information\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping class names to Detection objects. Only includes\n",
    "            classes that were detected in the image.\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the image file doesn't exist\n",
    "            \n",
    "        Example:\n",
    "            detections = detector.predict('gauge.jpg')\n",
    "            if 'tip' in detections:\n",
    "                tip_bbox = detections['tip'].bbox\n",
    "                confidence = detections['tip'].confidence\n",
    "        \"\"\"\n",
    "        \n",
    "        # Run inference\n",
    "        results = self.model(image)\n",
    "        \n",
    "        # Dictionary to store the highest confidence detection for each class\n",
    "        best_detections: Dict[str, Detection] = {}\n",
    "        \n",
    "        # Process all detections\n",
    "        for box in results[0].boxes:\n",
    "            class_id = int(box.cls[0])\n",
    "            class_name = self.class_names[class_id]\n",
    "            confidence = box.conf[0].item()\n",
    "            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "            \n",
    "            # Keep only the highest confidence detection for each class\n",
    "            if class_name not in best_detections or confidence > best_detections[class_name].confidence:\n",
    "                best_detections[class_name] = Detection(class_name, confidence, (x1, y1, x2, y2))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nFound {len(best_detections)} unique class(es):\")\n",
    "            for i, (class_name, detection) in enumerate(best_detections.items(), 1):\n",
    "                print(f\"\\nDetection {i}:\")\n",
    "                print(f\"  Class: {detection.class_name}\")\n",
    "                print(f\"  Confidence: {detection.confidence:.2%}\")\n",
    "                print(f\"  Box: [{detection.bbox[0]:.0f}, {detection.bbox[1]:.0f}, \"\n",
    "                      f\"{detection.bbox[2]:.0f}, {detection.bbox[3]:.0f}]\")\n",
    "        \n",
    "        return best_detections\n",
    "    \n",
    "    def show_results(self, image: PILImage) -> None:\n",
    "        \"\"\"\n",
    "        Run inference and display the image with bounding boxes.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the input image\n",
    "        \"\"\"\n",
    "        results = self.model(image)\n",
    "        results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5eddcd2-800c-4e08-bb69-336bc0652265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analog Gauge Reader Module\n",
    "\n",
    "This module extracts measurements from analog gauges by analyzing the detected\n",
    "bounding boxes of gauge components (center, tip, min, max markers).\n",
    "\n",
    "The measurement is calculated by:\n",
    "1. Finding the center point of each detected component\n",
    "2. Computing vectors from the gauge center to other components\n",
    "3. Calculating the angle between the needle tip and min marker\n",
    "4. Computing the maximum angle range between min and max markers\n",
    "5. Converting the angle to a measurement value using linear interpolation\n",
    "\n",
    "Usage:\n",
    "    from yolo import GaugeDetector\n",
    "    from gauge_reader import GaugeReader\n",
    "\n",
    "    # Detect gauge components\n",
    "    detector = GaugeDetector('yolo_best.pt')\n",
    "    detections = detector.predict('gauge_image.jpg')\n",
    "\n",
    "    # Read measurement\n",
    "    reader = GaugeReader(min_value=0, max_value=160, unit=\"PSI\")\n",
    "    measurement = reader.read_gauge(detections)\n",
    "    print(f\"Reading: {measurement:.2f} PSI\")\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Dict, Tuple, Optional\n",
    "#from yolo import Detection\n",
    "\n",
    "\n",
    "class Detection:\n",
    "    \"\"\"\n",
    "    Represents a single object detection with bounding box and metadata.\n",
    "\n",
    "    Attributes:\n",
    "        class_name (str): Name of the detected class\n",
    "        confidence (float): Detection confidence score (0-1)\n",
    "        bbox (Tuple[float, float, float, float]): Bounding box coordinates (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_name: str, confidence: float, bbox: Tuple[float, float, float, float]):\n",
    "        \"\"\"\n",
    "        Initialize a Detection object.\n",
    "\n",
    "        Args:\n",
    "            class_name: Name of the detected class\n",
    "            confidence: Confidence score between 0 and 1\n",
    "            bbox: Bounding box as (x1, y1, x2, y2) coordinates\n",
    "        \"\"\"\n",
    "        self.class_name = class_name\n",
    "        self.confidence = confidence\n",
    "        self.bbox = bbox\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f\"Detection(class='{self.class_name}', \"\n",
    "                f\"confidence={self.confidence:.2%}, \"\n",
    "                f\"bbox={[f'{x:.0f}' for x in self.bbox]})\")\n",
    "\n",
    "class GaugeReader:\n",
    "    \"\"\"\n",
    "    Reads measurements from analog gauges using detected component positions.\n",
    "\n",
    "    This class performs geometric calculations to determine the gauge reading\n",
    "    based on the positions of the needle tip relative to the min/max markers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_value: float = 0, max_value: float = 100, unit: str = \"units\"):\n",
    "        \"\"\"\n",
    "        Initialize the GaugeReader with gauge specifications.\n",
    "\n",
    "        Args:\n",
    "            min_value: The value corresponding to the minimum position on the gauge\n",
    "            max_value: The value corresponding to the maximum position on the gauge\n",
    "            unit: The unit of measurement (e.g., \"PSI\", \"°C\", \"RPM\")\n",
    "\n",
    "        Example:\n",
    "            reader = GaugeReader(min_value=0, max_value=160, unit=\"PSI\")\n",
    "        \"\"\"\n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "        self.unit = unit\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bbox_center(bbox: Tuple[float, float, float, float]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the center point of a bounding box.\n",
    "\n",
    "        Args:\n",
    "            bbox: Bounding box coordinates as (x1, y1, x2, y2)\n",
    "\n",
    "        Returns:\n",
    "            Numpy array containing the center coordinates [xc, yc]\n",
    "\n",
    "        Example:\n",
    "            center = get_bbox_center((0, 0, 10, 10))  # Returns [5.0, 5.0]\n",
    "        \"\"\"\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        xc = x1 + (x2 - x1) / 2\n",
    "        yc = y1 + (y2 - y1) / 2\n",
    "        return np.array([xc, yc])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_angle_between_vectors(v1: np.ndarray, v2: np.ndarray, in_degrees: bool = True) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the angle between two vectors.\n",
    "\n",
    "        Args:\n",
    "            v1: First vector as numpy array\n",
    "            v2: Second vector as numpy array\n",
    "            in_degrees: If True, return angle in degrees; if False, return in radians\n",
    "\n",
    "        Returns:\n",
    "            Angle between the vectors\n",
    "\n",
    "        Note:\n",
    "            The angle is always positive and between 0 and 180 degrees (or 0 and π radians).\n",
    "        \"\"\"\n",
    "        # Normalize vectors to avoid numerical issues\n",
    "        v1_norm = np.linalg.norm(v1)\n",
    "        v2_norm = np.linalg.norm(v2)\n",
    "\n",
    "        if v1_norm == 0 or v2_norm == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Calculate dot product and clamp to [-1, 1] to avoid numerical errors with arccos\n",
    "        cos_angle = np.clip(np.dot(v1, v2) / (v1_norm * v2_norm), -1.0, 1.0)\n",
    "        angle_rad = np.arccos(cos_angle)\n",
    "\n",
    "        if in_degrees:\n",
    "            return math.degrees(angle_rad)\n",
    "        return angle_rad\n",
    "\n",
    "    def read_gauge(self, detections: Dict[str, Detection], verbose: bool = True) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Calculate the gauge reading from detected components.\n",
    "\n",
    "        This method requires detections for 'center', 'tip', 'min', and 'max' classes.\n",
    "        It calculates the needle position relative to the gauge range and converts\n",
    "        it to a measurement value.\n",
    "\n",
    "        Args:\n",
    "            detections: Dictionary mapping class names to Detection objects\n",
    "            verbose: If True, print detailed calculation information\n",
    "\n",
    "        Returns:\n",
    "            The calculated measurement value, or None if required detections are missing\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If required detections are missing\n",
    "\n",
    "        Example:\n",
    "            measurement = reader.read_gauge(detections)\n",
    "            if measurement is not None:\n",
    "                print(f\"Gauge reading: {measurement:.2f} {reader.unit}\")\n",
    "        \"\"\"\n",
    "        # Check for required detections\n",
    "        required_classes = ['center', 'tip', 'min', 'max']\n",
    "        missing_classes = [cls for cls in required_classes if cls not in detections]\n",
    "\n",
    "        if missing_classes:\n",
    "            raise ValueError(\n",
    "                f\"Missing required detections: {', '.join(missing_classes)}. \"\n",
    "                f\"Cannot calculate gauge reading without all components.\"\n",
    "            )\n",
    "\n",
    "        # Extract center points of each bounding box\n",
    "        center = self.get_bbox_center(detections['center'].bbox)\n",
    "        tip = self.get_bbox_center(detections['tip'].bbox)\n",
    "        min_marker = self.get_bbox_center(detections['min'].bbox)\n",
    "        max_marker = self.get_bbox_center(detections['max'].bbox)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\nComponent Centers:\")\n",
    "            print(f\"  Center: {center}\")\n",
    "            print(f\"  Tip:    {tip}\")\n",
    "            print(f\"  Min:    {min_marker}\")\n",
    "            print(f\"  Max:    {max_marker}\")\n",
    "\n",
    "        # Calculate radius vectors from gauge center to each component\n",
    "        r_tip = tip - center\n",
    "        r_min = min_marker - center\n",
    "        r_max = max_marker - center\n",
    "\n",
    "        # Calculate the angle between min and max markers\n",
    "        # This represents the full range of the gauge\n",
    "        min_max_angle = self.get_angle_between_vectors(r_min, r_max, in_degrees=True)\n",
    "\n",
    "        # The gauge typically spans more than 180 degrees, so we take the reflex angle\n",
    "        # BUG FIX: Changed from 360 - angle to handle gauges that span less than 180 degrees\n",
    "        # We now check which interpretation makes sense based on gauge design\n",
    "        if min_max_angle < 180:\n",
    "            # Gauge spans less than 180 degrees - use the angle directly\n",
    "            max_angle = min_max_angle\n",
    "        else:\n",
    "            # This shouldn't happen with arccos, but kept for safety\n",
    "            max_angle = min_max_angle\n",
    "\n",
    "        # However, most analog gauges span more than 180 degrees (e.g., 270 degrees)\n",
    "        # So we typically want the reflex angle\n",
    "        # BUG FIX: The original calculation assumed this, which is correct for most gauges\n",
    "        max_angle = 360 - min_max_angle\n",
    "\n",
    "        # Calculate the angle between tip and min marker\n",
    "        # This represents the current needle position\n",
    "        tip_angle = self.get_angle_between_vectors(r_tip, r_min, in_degrees=True)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nAngle Calculations:\")\n",
    "            print(f\"  Full gauge range: {round(max_angle)}°\")\n",
    "            print(f\"  Needle position:  {round(tip_angle)}°\")\n",
    "\n",
    "        # Calculate the proportion of the gauge range covered by the needle\n",
    "        tip_proportion = tip_angle / max_angle\n",
    "\n",
    "        # Convert proportion to measurement value using linear interpolation\n",
    "        measurement = tip_proportion * (self.max_value - self.min_value) + self.min_value\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nGauge Reading:\")\n",
    "            print(f\"  Needle at {round(tip_proportion)} of full range\")\n",
    "            print(f\"  Measurement: {measurement:.1f} {self.unit}\")\n",
    "\n",
    "        return measurement\n",
    "\n",
    "    def read_gauge_from_image(self, image_path: str, detector, verbose: bool = True) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Convenience method to detect and read gauge in one step.\n",
    "\n",
    "        Args:\n",
    "            image_path: Path to the gauge image\n",
    "            detector: GaugeDetector instance for running inference\n",
    "            verbose: If True, print detailed information\n",
    "\n",
    "        Returns:\n",
    "            The calculated measurement value, or None if reading fails\n",
    "\n",
    "        Example:\n",
    "            from yolo import GaugeDetector\n",
    "            from gauge_reader import GaugeReader\n",
    "\n",
    "            detector = GaugeDetector('yolo_best.pt')\n",
    "            reader = GaugeReader(min_value=0, max_value=160, unit=\"PSI\")\n",
    "\n",
    "            measurement = reader.read_gauge_from_image('gauge.jpg', detector)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Run detection\n",
    "            detections = detector.predict(image_path, verbose=verbose)\n",
    "\n",
    "            # Calculate measurement\n",
    "            measurement = self.read_gauge(detections, verbose=verbose)\n",
    "\n",
    "            return measurement\n",
    "\n",
    "        except (ValueError, FileNotFoundError) as e:\n",
    "            print(f\"Error reading gauge: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "762f0d09-6771-40f2-ae78-512859250a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chris/Projects/Analog Gauge Monitoring/google-cloud-functions/process-image\n",
      "============================================================\n",
      "MODEL LOADED\n",
      "============================================================\n",
      "Classes: {0: 'center', 1: 'gauge', 2: 'max', 3: 'min', 4: 'tip'}\n",
      "Number of classes: 5\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GeForce GTX 1080 Ti which is of cuda capability 6.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/home/chris/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:304: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  warnings.warn(matched_cuda_warn.format(matched_arches))\n",
      "/home/chris/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:326: UserWarning: \n",
      "NVIDIA GeForce GTX 1080 Ti with CUDA capability sm_61 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the NVIDIA GeForce GTX 1080 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(os.getcwd())\n\u001b[32m      2\u001b[39m detector = GaugeDetector(os.path.join(\u001b[33m\"\u001b[39m\u001b[33m../\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m../\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myolo_best.pt\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results = \u001b[43mdetector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mGaugeDetector.predict\u001b[39m\u001b[34m(self, image, verbose)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03mRun inference on an image and return the highest confidence detection for each class.\u001b[39;00m\n\u001b[32m     97\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m        confidence = detections['tip'].confidence\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Dictionary to store the highest confidence detection for each class\u001b[39;00m\n\u001b[32m    124\u001b[39m best_detections: Dict[\u001b[38;5;28mstr\u001b[39m, Detection] = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:182\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, source, stream, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    156\u001b[39m     source: \u001b[38;5;28mstr\u001b[39m | Path | \u001b[38;5;28mint\u001b[39m | Image.Image | \u001b[38;5;28mlist\u001b[39m | \u001b[38;5;28mtuple\u001b[39m | np.ndarray | torch.Tensor = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    157\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    158\u001b[39m     **kwargs: Any,\n\u001b[32m    159\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    160\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \u001b[33;03m    This method simplifies the process of making predictions by allowing the model instance to be called directly\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m \u001b[33;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:540\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    539\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:225\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:38\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     42\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:306\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;66;03m# Warmup model\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done_warmup:\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwarmup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtriton\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28mself\u001b[39m.done_warmup = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[38;5;28mself\u001b[39m.seen, \u001b[38;5;28mself\u001b[39m.windows, \u001b[38;5;28mself\u001b[39m.batch = \u001b[32m0\u001b[39m, [], \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/ultralytics/nn/autobackend.py:878\u001b[39m, in \u001b[36mAutoBackend.warmup\u001b[39m\u001b[34m(self, imgsz)\u001b[39m\n\u001b[32m    876\u001b[39m im = torch.empty(*imgsz, dtype=torch.half \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp16 \u001b[38;5;28;01melse\u001b[39;00m torch.float, device=\u001b[38;5;28mself\u001b[39m.device)  \u001b[38;5;66;03m# input\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# warmup model\u001b[39;00m\n\u001b[32m    879\u001b[39m     warmup_boxes = torch.rand(\u001b[32m1\u001b[39m, \u001b[32m84\u001b[39m, \u001b[32m16\u001b[39m, device=\u001b[38;5;28mself\u001b[39m.device)  \u001b[38;5;66;03m# 16 boxes works best empirically\u001b[39;00m\n\u001b[32m    880\u001b[39m     warmup_boxes[:, :\u001b[32m4\u001b[39m] *= imgsz[-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/ultralytics/nn/autobackend.py:658\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:137\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:154\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:176\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    177\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/ultralytics/nn/modules/conv.py:89\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     81\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[32m     82\u001b[39m \n\u001b[32m     83\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Analog Gauge Monitoring/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "detector = GaugeDetector(os.path.join(\"../\", \"../\",\"models\", \"yolo_best.pt\"))\n",
    "results = detector(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2408c9c-b764-439f-8ced-823f391f8192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
